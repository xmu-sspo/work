1、8个网站中，6个的日期存在url中，1个只精确到月，1个url中不包含日期。
爬取计划：
对日期存在url中的：启动前把历史新闻都爬取好，代码改成爬取当日新闻，这样每日更新就只需爬取当日新闻即可。
	效率上，每个网站大概每日新闻24小时内有100-200条左右，平均120-160条。爬取时间为1分钟到1分半钟。
	（每月1000-4000条不等，平均有2000条，少的也有几百条，有些网站越久远的数据越少只有100条如观察者，有些则保留了大量的久远数据如新浪网4000条）
月份存在url中的：正则匹配精确到月，每日更新时在爬取发表时间后进行判断，取当日新闻。
	效率：抵死5分钟（未做实际测试）
对日期不包含在url中的：在爬取发表时间后进行判断。正则匹配不用改变。
	效率：抵死5分钟（未做实际测试）

海诗那边也有8个网站，每日新闻可能大概30分钟-60分钟可爬取完成一次。（恩历史数据一定要先爬好，十分费时）
还有一个问题，因为id自增的关系，对于insert error的记录仍然占用了数据表的id，会导致id可能出现断层和不连续的问题！！！！

2、新闻的发表时间从00:00:00到23:59:59不等。
因每日23:50:00清空this_day的数据，为避免漏掉23:50分之后的数据，因此在每日第一次爬取当日新闻时应当把前一天的新闻再爬一遍。
-> 综合以上设计：
1）历史数据以前：
	代码上取一年期的正则表达式，存入月份表
2）已爬取历史数据以后：
	①sql操作从月份表取30天内的数据存入this_month表
	②代码上包含当日和前一日的正则表达式匹配，前一日的存入月份表，当日的存入月份表、this_day表、this_month表。
	
3、爬取次数设计：
方案A（每日1:00一次、6:00-23:00不间断爬）
方案B：
1）每日1:00爬一次，补充缺的前日数据
2）6:00爬一次
3）7:00爬一次（7-9点不间断爬？）
4）14:00爬一次
5）17点一次
6）22:00一次

4、运行速度优化策略：
1）用212机房电脑试，可以开启多个命令窗口跑，而且速度还行
2）前期将历史数据爬好，正式启动时只爬取每日新闻
前期为保证id顺序增长，插入前仍需要select，防止插入失败而id出现断层
后期select计数在2000-6000内，应该还是可以的吧。。。
3）set集合只在每个scrapy中用，只能保证爬取这一个网站过程中不会有重复，而这个爬虫结束以后再次启动则set会清空重来。因此这个只能在前期爬取历史数据时有用

5、剩下哪些任务？
1）全部网站历史数据爬取
1）新闻content处理
2）检索 （建一个Git团队，共享代码）
3）定时建表删表
4）定时爬虫/不间断爬虫？
下周再做一周，周末不不不？开会，8 9 周复习考试休息；第9周周末 11.17开会。





